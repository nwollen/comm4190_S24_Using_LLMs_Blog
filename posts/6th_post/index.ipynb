{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d78c986d-f035-498e-a275-338224773614",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Who's the True Expert?\"\n",
    "description: \"In our class project, we wanted to see if we could tell who's really the expert in a topic versus just ChatGPT. Let's see how easy it is to figure out who the real expert is!\"\n",
    "author: \"Norabug\"\n",
    "date: \"4/30/2024\"\n",
    "categories:\n",
    "  - track\n",
    "  - basketball\n",
    "  - consulting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee8da18-c68d-44d5-852d-764de1a74f03",
   "metadata": {},
   "source": [
    "# The game of basketball, the sport of track. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aba1dc-6776-414d-9246-2b61b90920ff",
   "metadata": {},
   "source": [
    "![](trackbball.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444e63e-d3c8-4ad6-b075-166c0db3abe4",
   "metadata": {},
   "source": [
    "# \n",
    "Reflection: AI, Expertise, and Being Human\n",
    "\n",
    "We presented two sets of descriptions of track and field events to our classmates, prompting them to discern which one was crafted by a genuine expert and which one was generated by AI. I prompted ChatGPT to try and guess who was the expert and who was not. I copied and pasted an example of my slide, as an expert talking about track, and an example of Jason's slide talking about track using ChatGPT. \n",
    "\n",
    "## The Experiment\n",
    "\n",
    "Jason prompted ChatGPT to compose a description of track and field events, while I drew upon my own knowledge as a genuine enthusiast of the sport. The goal was twofold: to see if ChatGPT could convincingly mimic expertise, and to spark a discussion about the intersection of AI and human knowledge.\n",
    "\n",
    "## The Content\n",
    "\n",
    "### My Expertise \n",
    "\n",
    "```\n",
    "- Track is split up into jumps, vault, throws, sprints, hurdles, mid distance, and distance. \n",
    "- Jumps consist of high jump, long jump, triple jump, and pole vault. \n",
    "- Throws consist of javelin, shot put, hammer throw, and discus\n",
    "- Sprints consist of 60 (indoor), 100, 200, 300 (indoor), 400\n",
    "- Hurdles consist of all sprint events except 300\n",
    "- Mid-D means 800, 1500, 1600 (indoor)\n",
    "- Distance is 3000 (indoor), 5000, 6000 (cross country), 10000\n",
    "\n",
    "\n",
    "- Sprint/jump events require more power\n",
    "- Athletes work on explosiveness, short but heavy loads\n",
    "- Hurdles works on running through hurdles, mobility, power \n",
    "- Throws works on muscle mass\n",
    "- Lifts very heavy, have to be able to throw something heavy \n",
    "- Mid-D is a combination of anaerobic (short and fast) and aerobic (long cardio)\n",
    "- Distance is all aerobic (continuous running needing oxygen to power)\n",
    "- Lifting is long reps of light weight\n",
    "- Pole vault does a lot of gymnastics \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1b673-6bef-4916-add0-bf14f342c3d9",
   "metadata": {},
   "source": [
    "While our experiment challenged the boundaries between human-generated and AI-generated content, it reaffirmed the truth that expertise is rooted in the depths of human understanding. Despite AI's ability to simulate human-like responses, there's an intangible quality to human expertise that can't be replicated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eabc07-55f7-4a66-a4f5-bb57bc2aa4cf",
   "metadata": {},
   "source": [
    "# Reflection: AI, Expertise, and Being Human\n",
    "\n",
    "We presented two sets of descriptions of track and field events to our classmates, prompting them to discern which one was crafted by a genuine expert and which one was generated by AI. I prompted ChatGPT to try and guess who was the expert and who was not. I copied and pasted an example of my slide, as an expert talking about track, and an example of Jason's slide talking about track using ChatGPT.\n",
    "\n",
    "## The Experiment\n",
    "\n",
    "Jason prompted ChatGPT to compose a description of track and field events, while I drew upon my own knowledge as a genuine enthusiast of the sport. The goal was twofold: to see if ChatGPT could convincingly mimic expertise, and to spark a discussion about the intersection of AI and human knowledge.\n",
    "\n",
    "## My Expertise \n",
    "\n",
    "- Track is split up into jumps, vault, throws, sprints, hurdles, mid distance, and distance. \n",
    "- Jumps consist of high jump, long jump, triple jump, and pole vault. \n",
    "- Throws consist of javelin, shot put, hammer throw, and discus\n",
    "- Sprints consist of 60 (indoor), 100, 200, 300 (indoor), 400\n",
    "- Hurdles consist of all sprint events except 300\n",
    "- Mid-D means 800, 1500, 1600 (indoor)\n",
    "- Distance is 3000 (indoor), 5000, 6000 (cross country), 10000\n",
    "\n",
    "## Jason's ChatGPT Crafted \"Expertise\"\n",
    "\n",
    "- Sprints: These are short-distance races, typically 100 meters, 200 meters, and 400 meters.\n",
    "- Middle-Distance: Events like the 800 meters and 1500 meters require a balance of speed and endurance.\n",
    "- Long-Distance: Races such as the 3000 meters, 5000 meters, and 10,000 meters test endurance and pacing abilities.\n",
    "\n",
    "- Hurdles: Occurs in events like the 100 meters hurdles and the 400 meters hurdles\n",
    "- Long Jump: Athletes sprint down a runway and jump into a sandpit, aiming for maximum distance.\n",
    "- Triple Jump: Similar to the long jump but with three phases of jumps (hop, step, jump) before landing in the sandpit.\n",
    "- High Jump: Athletes jump over a horizontal bar using a technique that involves clearing the bar without knocking it off.\n",
    "- Pole Vault: Athletes use a pole to vault themselves over a high bar\n",
    "\n",
    "- Shot Put: Athletes throw a heavy metal ball (the shot) for distance, using a pushing motion.\n",
    "- Discus Throw: Athletes spin and release a discus, aiming for maximum distance.\n",
    "- Javelin Throw: Athletes throw a spear-like javelin for distance, using a run-up and specific throwing technique.\n",
    "- Hammer Throw: Athletes spin and release a heavy hammer attached to a wire, aiming for distance. \n",
    "\n",
    "## Class Reaction\n",
    "\n",
    "Generally, the class knew it was me who was the track expert. One person added that they only knew because I walk around in a Penn Track & Field bag. That makes me think that maybe ChatGPT can simulate human expertise. For other classmates' presentations, some people mistakenly thought the ChatGPT created expertise was the true human expertise.\n",
    "\n",
    "## Can ChatGPT correctly guess who is the expert and who has used AI?\n",
    "\n",
    "### I prompted Chat to try and decipher which one of us is the track expert, and which one is not (my track expertise as 1st and Jason's fake ChatGPT expertise as 2nd. Here's what it found...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa359a-997c-46f9-a366-952a7dd0efbb",
   "metadata": {},
   "source": [
    "Based on the provided examples, it seems that the first person is Jason and the second person is the real expert. The first person's description lacks some specificity and depth, using more general terms like 'work on explosiveness' and 'lifts very heavy.' In contrast, the second person's description provides specific distances for each type of race, indicating a deeper understanding of track and field events. Additionally, the second person uses terms like 'balance of speed and endurance' and 'tests endurance and pacing abilities,' which are more indicative of someone with genuine expertise in athletics.\n",
    "\n",
    "---\n",
    "\n",
    "Chat, I'm sorry to say, but you guessed wrong. I wrote the first section as an expert, and the second section was written by Jason using ChatGPT. What do you have to say for yourself?\n",
    "\n",
    "Chat's response was as follows...\n",
    "\n",
    "\"My apologies for the incorrect guess! It seems that Jason's use of ChatGPT to generate the second section was quite convincing. This highlights the challenge of accurately discerning between genuine expertise and AI-generated content, even for an AI like myself. It's a testament to the advancements in natural language processing and the potential for AI to simulate human-like responses.\n",
    "\n",
    "However, it's also important to acknowledge that while AI can generate content, it may not always capture the depth, nuance, and personal experience that come with genuine human expertise. This experiment underscores the value of human insight and intuition, which can't be fully replicated by AI.\"\n",
    "\n",
    "---\n",
    "\n",
    "While our experiment challenged the boundaries between human-generated and AI-generated content, it reaffirmed the truth that expertise is rooted in the depths of human understanding. Despite AI's ability to simulate human-like responses, there's an intangible quality to human expertise that can't be replicated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3410f08-429f-4b9f-80df-8a4f943532f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec079a7-7f5d-418d-820a-03814b1e8f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3.11 (COMM4190)",
   "language": "python",
   "name": "comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
